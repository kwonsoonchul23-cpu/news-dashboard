import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm # í°íŠ¸ ê°•ì œ ì ìš©ì„ ìœ„í•´ ì¶”ê°€
from wordcloud import WordCloud
from transformers import pipeline
import os
import time
import datetime
import re
import networkx as nx
import plotly.express as px
from sklearn.feature_extraction.text import CountVectorizer

# ---------------------------------------------------------
# 1. ê¸°ë³¸ ì„¤ì • ë° í°íŠ¸ ì„¸íŒ… (í´ë¼ìš°ë“œ í™˜ê²½ í°íŠ¸ ì—ëŸ¬ ì™„ë²½ í•´ê²°)
# ---------------------------------------------------------
st.set_page_config(page_title="AI íƒì‚¬ë³´ë„ ì‹œìŠ¤í…œ (ìµœì¢… ê³ ë„í™”)", layout="wide")

# í´ë”ì— ì˜¬ë¦° malgun.ttfë¥¼ ê·¸ë˜í”„ í°íŠ¸ë¡œ ê°•ì œ ì£¼ì…í•©ë‹ˆë‹¤.
if os.path.exists('malgun.ttf'):
    fm.fontManager.addfont('malgun.ttf')
    plt.rcParams['font.family'] = fm.FontProperties(fname='malgun.ttf').get_name()
else:
    plt.rcParams['font.family'] = 'Malgun Gothic'
    
plt.rcParams['axes.unicode_minus'] = False

st.markdown("""
    <style>
    .stApp { background-color: #F5F7F9; }
    h1 { color: #2C3E50; font-family: 'Malgun Gothic', sans-serif; font-weight: 800; border-bottom: 2px solid #3498DB; padding-bottom: 10px; }
    div[data-testid="stMetric"] { background-color: #FFFFFF; border: 1px solid #E0E0E0; border-radius: 10px; box-shadow: 2px 2px 5px rgba(0,0,0,0.05); padding: 15px; }
    </style>
    """, unsafe_allow_html=True)

# ---------------------------------------------------------
# 2. ë°ì´í„° & ëª¨ë¸ ë¡œë“œ
# ---------------------------------------------------------
@st.cache_data
def load_data():
    try:
        df = pd.read_excel("news_result_final.xlsx")
        if 'ì¼ì' in df.columns:
            df['ì¼ì'] = pd.to_datetime(df['ì¼ì'].astype(str).str[:8], errors='coerce')
            df = df.dropna(subset=['ì¼ì'])
        return df
    except Exception as e:
        return pd.DataFrame()

@st.cache_resource
def load_model():
    return pipeline("sentiment-analysis", model="matthewburke/korean_sentiment")

df = load_data()
classifier = load_model()

# ---------------------------------------------------------
# 3. ì‚¬ì´ë“œë°” (ì¹œì ˆí•œ ì„¤ëª… ë° UI ê°œì„ )
# ---------------------------------------------------------
st.sidebar.title("ğŸ›ï¸ AI ë¶„ì„ & ì „ì²˜ë¦¬ ì˜µì…˜")

st.sidebar.markdown("### ğŸ” íŠ¹ì • í‚¤ì›Œë“œ í•„í„°ë§")
st.sidebar.caption(
    "ğŸ’¡ **ì™œ ì“°ë‚˜ìš”?**<br>"
    "ì „ì²´ ê¸°ì‚¬ ì¤‘ íŠ¹ì • ì£¼ì œ(ì˜ˆ: 'ì¼ìë¦¬', 'ë²”ì£„', 'êµìœ¡')ë§Œ ì¢í˜€ì„œ ë¶„ì„í•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.<br>"
    "ë‹¨ì–´ë¥¼ ì…ë ¥í•˜ë©´ í•´ë‹¹ ë‹¨ì–´ê°€ í¬í•¨ëœ ê¸°ì‚¬ë§Œ ì¶”ë ¤ë‚´ì–´ ì—¬ë¡  íŠ¸ë Œë“œì™€ ê¸/ë¶€ì • ë¹„ìœ¨ì„ ë‹¤ì‹œ ê³„ì‚°í•©ë‹ˆë‹¤.", 
    unsafe_allow_html=True
)
target_keyword = st.sidebar.text_input("ë¶„ì„í•  ë‹¨ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: êµìœ¡, ì¼ìë¦¬, ë²”ì£„")

st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ›‘ ìì—°ì–´ ì „ì²˜ë¦¬ (ë¶ˆìš©ì–´ ì œê±°)")
st.sidebar.caption("ì›Œë“œí´ë¼ìš°ë“œ ë“±ì—ì„œ ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´(ë…¸ì´ì¦ˆ)ë¥¼ ê±¸ëŸ¬ëƒ…ë‹ˆë‹¤.")
default_stopwords = "ê¸°ì, ë‰´ìŠ¤, ì˜¤ëŠ˜, ë°œë‹¬ì¥ì• ì¸, ë°œë‹¬ì¥ì• , ì¥ì• ì¸, ìƒê°, ì‚¬ëŒ, ì‚¬íšŒ, ìµœê·¼, ì‹œê°„, ì§€ì›, ì„¼í„°, ëŒ€í•œ, ìœ„í•´"
user_stopwords = st.sidebar.text_area("ë¶ˆìš©ì–´ ëª©ë¡ (ì‰¼í‘œë¡œ êµ¬ë¶„)", value=default_stopwords)
stopword_list = [word.strip() for word in user_stopwords.split(',')]

st.sidebar.markdown("---")
st.sidebar.subheader("âš™ï¸ ë„¤íŠ¸ì›Œí¬ & ì›Œë“œí´ë¼ìš°ë“œ ì„¤ì •")
max_words = st.sidebar.slider("ë¶„ì„í•  í•µì‹¬ ë‹¨ì–´ ìˆ˜", 5, 20, 10)
min_word_length = st.sidebar.slider("ìµœì†Œ ë‹¨ì–´ ê¸¸ì´", 1, 5, 2)

st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ“… ë¶„ì„ ê¸°ê°„ ì„¤ì •")
if not df.empty and 'ì¼ì' in df.columns:
    min_date = df['ì¼ì'].min().date()
    max_date = df['ì¼ì'].max().date()
    
    st.sidebar.info(f"ğŸ“Œ **ì„ íƒ ê°€ëŠ¥ ê¸°ê°„ (ìƒ/í•˜í•œì„ )**\n\nìµœì €: {min_date}\n\nìµœê³ : {max_date}")
    
    col1, col2 = st.sidebar.columns(2)
    with col1: start_date = st.date_input("ì‹œì‘ì¼", value=min_date, min_value=min_date, max_value=max_date)
    with col2: end_date = st.date_input("ì¢…ë£Œì¼", value=max_date, min_value=min_date, max_value=max_date)
else:
    start_date, end_date = None, None

# ---------------------------------------------------------
# 4. ë©”ì¸ ëŒ€ì‹œë³´ë“œ
# ---------------------------------------------------------
st.title("ğŸ“° AI ê¸°ë°˜ ë°œë‹¬ì¥ì• ì¸ ë‰´ìŠ¤ ì‹¬ì¸µ ë¶„ì„ê¸°")

with st.spinner('â³ ë°ì´í„°ë¥¼ ì •êµí•˜ê²Œ ì¬ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'):
    time.sleep(0.3)

    if not df.empty and 'ì¼ì' in df.columns:
        mask = (df['ì¼ì'].dt.date >= start_date) & (df['ì¼ì'].dt.date <= end_date)
        global_df_filtered = df.loc[mask].copy()
        if target_keyword:
            global_df_filtered = global_df_filtered[global_df_filtered['ì œëª©'].str.contains(target_keyword, na=False)]
    else:
        global_df_filtered = pd.DataFrame()

    total = len(global_df_filtered)
    pos = len(global_df_filtered[global_df_filtered['ê°ì„±'] == 'ê¸ì •']) if total > 0 else 0
    neg = len(global_df_filtered[global_df_filtered['ê°ì„±'] == 'ë¶€ì •']) if total > 0 else 0

    col1, col2, col3, col4 = st.columns(4)
    col1.metric("ì¡°ê±´ ë¶€í•© ê¸°ì‚¬", f"{total:,} ê±´")
    col2.metric("ê¸ì • ì—¬ë¡ ", f"{pos:,} ê±´", f"{round(pos/total*100, 1) if total>0 else 0}%")
    col3.metric("ë¶€ì • ì—¬ë¡ ", f"{neg:,} ê±´", f"-{round(neg/total*100, 1) if total>0 else 0}%")
    col4.metric("ë¶„ì„ íƒ€ê²Ÿ", target_keyword if target_keyword else "ì „ì²´ ì œëª©")

    st.markdown("---")

    processed_docs = []
    
    if total > 0:
        for title in global_df_filtered['ì œëª©'].dropna().astype(str):
            clean_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', title)
            tokens = clean_text.split()
            final_tokens = [word for word in tokens if len(word) >= min_word_length and word not in stopword_list]
            if final_tokens:
                processed_docs.append(" ".join(final_tokens))

    st.subheader("ğŸ•¸ï¸ í•µì‹¬ í‚¤ì›Œë“œ ì—°ê´€ì„± ë° ì‹¬ì¸µ ë¶„ì„")
    st.caption("ì„ (Edge)ì´ êµµì„ìˆ˜ë¡ ì–¸ë¡ ì—ì„œ ë‘ ë‹¨ì–´ë¥¼ ê¸°ì‚¬ ì œëª©ì— í•¨ê»˜(ë™ì‹œì—) ë§ì´ ì‚¬ìš©í–ˆë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.")
    tab_wordcloud, tab_network, tab_heatmap = st.tabs(["â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ", "ğŸŒ ì—°ê´€ì„± ë„¤íŠ¸ì›Œí¬", "ğŸŸª ìœ ì‚¬ë„ íˆíŠ¸ë§µ"])

    if len(processed_docs) > 5:
        cv = CountVectorizer(max_features=max_words)
        dtm = cv.fit_transform(processed_docs)
        df_dtm = pd.DataFrame(dtm.toarray(), columns=cv.get_feature_names_out())
        corr_matrix = df_dtm.corr().fillna(0)

        with tab_wordcloud:
            text_for_wc = " ".join(processed_docs)
            font_path_wc = 'malgun.ttf' if os.path.exists('malgun.ttf') else None 
            wc = WordCloud(width=800, height=350, background_color='white', font_path=font_path_wc, colormap='viridis').generate(text_for_wc)
            fig_wc, ax = plt.subplots(figsize=(10, 4))
            ax.imshow(wc, interpolation='bilinear')
            ax.axis('off')
            st.pyplot(fig_wc)

        with tab_network:
            G = nx.Graph()
            words = corr_matrix.columns
            for word in words: G.add_node(word)
            for i in range(len(words)):
                for j in range(i+1, len(words)):
                    weight = corr_matrix.iloc[i, j]
                    if weight > 0.1: 
                        G.add_edge(words[i], words[j], weight=weight)
            
            fig_net, ax = plt.subplots(figsize=(10, 6))
            pos_net = nx.spring_layout(G, k=0.5, seed=42)
            
            # [ë””ìì¸ ìˆ˜ì •] ë™ê·¸ë¼ë¯¸ëŠ” ì—°í•œ ë³´ë¼ìƒ‰, ê¸€ìëŠ” ì§„í•œ ê²€ì€ìƒ‰ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ê°€ë…ì„± ê·¹ëŒ€í™”
            nx.draw_networkx_nodes(G, pos_net, node_size=2500, node_color='#E8EAF6', edgecolors='#7B68EE', linewidths=2, ax=ax)
            nx.draw_networkx_edges(G, pos_net, width=[G[u][v]['weight']*5 for u,v in G.edges()], edge_color='#BDBDBD', ax=ax)
            # ê¸€ìê°€ ì˜ ë³´ì´ë„ë¡ font_colorë¥¼ 'black'ìœ¼ë¡œ ê°•ì œ ì§€ì •
            nx.draw_networkx_labels(G, pos_net, font_size=13, font_color='black', font_weight='bold', ax=ax)
            
            plt.axis('off')
            st.pyplot(fig_net)

        with tab_heatmap:
            fig_heat = px.imshow(corr_matrix, text_auto=".2f", aspect="auto", color_continuous_scale='Purples')
            fig_heat.update_layout(margin=dict(l=20, r=20, t=20, b=20))
            st.plotly_chart(fig_heat, use_container_width=True)
            
    else:
        st.info("ì—°ê´€ì„± ë¶„ì„ì„ ìˆ˜í–‰í•˜ê¸°ì—ëŠ” í•„í„°ë§ëœ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ì¡°ê±´ ë²”ìœ„ë¥¼ ë„“í˜€ì£¼ì„¸ìš”.")

# ---------------------------------------------------------
# 5. ì‹¤ì‹œê°„ AI íŒ©íŠ¸ì²´í¬
# ---------------------------------------------------------
st.markdown("---")
st.subheader("ğŸ•µï¸â€â™€ï¸ ì‹¤ì‹œê°„ AI íŒ©íŠ¸ì²´í¬ & í¸í–¥ì„± íƒì§€")

tab1, tab2 = st.tabs(["ğŸ“œ í˜„ì¬ í•„í„°ë§ëœ ê¸°ì‚¬ ëª©ë¡ì—ì„œ ê²€ì¦", "âœï¸ ì§ì ‘ ì…ë ¥í•´ì„œ ê²€ì¦"])
target_article = ""
target_url = "" 

with tab1:
    if not global_df_filtered.empty:
        top_articles = global_df_filtered.sort_values(by='ì¼ì', ascending=False).head(50)
        has_publisher = 'ì–¸ë¡ ì‚¬' in top_articles.columns
        
        url_col = None
        if 'URL' in top_articles.columns: 
            url_col = 'URL'
        elif 'ê¸°ì‚¬ URL' in top_articles.columns: 
            url_col = 'ê¸°ì‚¬ URL'
        
        display_dict = {}
        for _, row in top_articles.iterrows():
            date_str = row['ì¼ì'].strftime('%Y-%m-%d')
            publisher_str = row['ì–¸ë¡ ì‚¬'] if has_publisher else "ì•Œìˆ˜ì—†ìŒ"
            title_str = str(row['ì œëª©'])
            
            url_str = str(row[url_col]) if url_col and pd.notna(row[url_col]) else ""
            
            display_text = f"[{date_str}] ({publisher_str}) {title_str}"
            display_dict[display_text] = {
                "title": title_str,
                "url": url_str
            }
        
        selected_option = st.selectbox(
            "ê²€ì¦í•  ê¸°ì‚¬ë¥¼ ì„ íƒí•˜ì„¸ìš” (ìµœì‹ ìˆœ 50ê±´):", 
            list(display_dict.keys())
        )
        
        if selected_option: 
            target_article = display_dict[selected_option]["title"]
            target_url = display_dict[selected_option]["url"] 
    else:
        st.warning("ì¡°ê±´ì— ë§ëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

with tab2:
    input_text = st.text_area("ì˜ì‹¬ë˜ëŠ” ê¸°ì‚¬ ì œëª©ì´ë‚˜ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”:", height=100)
    if input_text: 
        target_article = input_text
        target_url = ""

if st.button("ğŸ” íŒ©íŠ¸ì²´í¬ ì‹œì‘"):
    if target_article:
        with st.spinner("â³ AI ë¶„ì„ ì¤‘..."):
            time.sleep(0.5)
            result = classifier(target_article)[0]
            label = "ê¸ì •" if result['label'] == 'LABEL_1' else "ë¶€ì •"
            score = round(result['score'] * 100, 2)
            
            if score >= 90:
                level_text = "ë§¤ìš° ê°•í•¨ (ì£¼ì˜ ìš”ë§)"
                social_guide = "ê¸°ì‚¬ì— ìê·¹ì ì¸ ë‹¨ì–´ë‚˜ ê°ì •ì  í‘œí˜„ì´ ì§‘ì¤‘ì ìœ¼ë¡œ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. <b>ì‚¬íšŒì  í¸ê²¬ì„ ì¡°ì¥í•˜ê±°ë‚˜ ê³¼ì¥ëœ ì–´ë·°ì§• ê¸°ì‚¬</b>ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ, íƒ€ ì–¸ë¡ ì‚¬ì˜ íŒ©íŠ¸ì²´í¬ê°€ ê°•ë ¥íˆ ê¶Œì¥ë©ë‹ˆë‹¤."
            elif score >= 70:
                level_text = "ëšœë ·í•œ ë…¼ì¡°"
                social_guide = "ê¸°ìì˜ ì£¼ê´€ì´ë‚˜ íŠ¹ì • ì‹œê°ì´ ëšœë ·í•˜ê²Œ ë°˜ì˜ëœ ê¸°ì‚¬ì…ë‹ˆë‹¤. ê°ê´€ì  ì‚¬ì‹¤ê³¼ ì˜ê²¬ì„ ë¶„ë¦¬í•˜ì—¬ <b>ê· í˜• ì¡íŒ ì‹œê°</b>ìœ¼ë¡œ ìˆ˜ìš©í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤."
            else:
                level_text = "ë¹„êµì  ì¤‘ë¦½/ê°ê´€ì "
                social_guide = "ê°ì •ì ì¸ ë‹¨ì–´ ì‚¬ìš©ì´ ì ê³ , <b>ì‚¬ì‹¤(Fact) ì „ë‹¬ ìœ„ì£¼</b>ë¡œ ê±´ì¡°í•˜ê²Œ ì‘ì„±ë˜ì—ˆì„ í™•ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. ë¹„êµì  ê°ê´€ì ì¸ ì •ë³´ë¡œ ë°›ì•„ë“¤ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            
            col_title, col_btn = st.columns([4, 1])
            with col_title:
                st.markdown(f"**ë¶„ì„ ëŒ€ìƒ:** {target_article}")
            with col_btn:
                if target_url and target_url.startswith("http"):
                    st.link_button("ğŸ”— ê¸°ì‚¬ ì›ë¬¸ ë³´ê¸°", target_url)
            
            if label == "ë¶€ì •":
                st.error(f"ğŸš¨ **[ë¶€ì •/ë¹„íŒ í¸í–¥ì„±]** AI í™•ì‹ ë„: {score}% ({level_text})")
            else:
                st.success(f"âœ… **[ê¸ì •/í¬ë§ í¸í–¥ì„±]** AI í™•ì‹ ë„: {score}% ({level_text})")
                
            st.info(f"ğŸ’¡ **AI íŒë‹¨ ê°€ì´ë“œ (Media Literacy):** \n\n"
                    f"ë‹¨ìˆœíˆ ê¸°ì‚¬ì˜ ë‚´ìš©ì´ 'ì¢‹ë‹¤/ë‚˜ì˜ë‹¤'ë¥¼ ë„˜ì–´, ì´ ê¸°ì‚¬ê°€ ì–¸ë¡ ì˜ ê°ê´€ì„±ì„ ì–¼ë§ˆë‚˜ ìœ ì§€í•˜ê³  ìˆëŠ”ì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” ì§€í‘œì…ë‹ˆë‹¤. AIê°€ ì–¸ì–´ì  íŒ¨í„´ì„ ë¶„ì„í•œ ê²°ê³¼, ì´ ê¸°ì‚¬ëŠ” **{label} í”„ë ˆì„**ì— ì†í•©ë‹ˆë‹¤.\n\n"
                    f"**ğŸ“Œ ìƒì‹ì  í•´ì„:** {social_guide}")
    else:
        st.warning("ê¸°ì‚¬ë¥¼ ì„ íƒí•˜ê±°ë‚˜ ì…ë ¥í•´ì£¼ì„¸ìš”.")
